{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "https://medium.com/@hhpatil001/transformers-from-scratch-in-simple-python-part-i-b290760c1040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "figsize=(14, 4)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt, log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = 'I love data science.'\n",
    "print(tokenizer(text, add_special_tokens=False, return_tensors='pt'))\n",
    "inputs = tokenizer(text, add_special_tokens=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeds = token_embeddings(inputs.input_ids)\n",
    "print(input_embeds.size())\n",
    "#Batch size, sequence length, hidden dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
    "    # query/key/value shapes: (batch, seq_len, head_dim)\n",
    "    dim_k = query.size(-1)\n",
    "    # compute scores: (batch, seq_len, seq_len)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        # mask: 1 for positions to mask; convert to large negative number\n",
    "        scores = scores.masked_fill(mask.bool(), float(\"-1e9\"))\n",
    "\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        weights = dropout(weights)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "One attention head has the tendency to only focus on a certain feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\tdef __init__(self, embed_dim, head_dim, attention_dropout=0.0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.q = nn.Linear(embed_dim, head_dim)\n",
    "\t\tself.k = nn.Linear(embed_dim, head_dim)\n",
    "\t\tself.v = nn.Linear(embed_dim, head_dim)\n",
    "\t\tself.attention_dropout = nn.Dropout(attention_dropout) if attention_dropout > 0.0 else None\n",
    "\t\n",
    "\tdef forward(self, hidden_state, mask=None):\n",
    "\t\tattention_outputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), mask=mask, dropout=self.attention_dropout)\n",
    "\t\treturn attention_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tembed_dim = config.hidden_size\n",
    "\t\tnum_heads = config.num_attention_heads\n",
    "\t\thead_dim = embed_dim // num_heads\n",
    "\t\tself.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim, attention_dropout=config.attention_probs_dropout_prob) for _ in range(num_heads)])\n",
    "\t\tself.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\t\tself.out_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\t\n",
    "\tdef forward(self, hidden_state, mask):\n",
    "\t\tx = torch.cat([h(hidden_state, mask=mask) for h in self.heads], dim=-1)\n",
    "\t\tx = self.output_linear(x)\n",
    "\t\tx = self.out_dropout(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention = MultiHeadAttention(config)\n",
    "attention_output = multihead_attention(input_embeds, None)\n",
    "print(attention_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "\t\tself.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\t\tself.gelu = nn.GELU()\n",
    "\t\tself.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.linear_1(x)\n",
    "\t\tx = self.gelu(x)\n",
    "\t\tx = self.linear_2(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attention_output)\n",
    "print(ff_outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.max_len = config.max_position_embeddings\n",
    "\t\tself.hidden_size = config.hidden_size\n",
    "\n",
    "\t\tpos_enc = torch.zeros(self.max_len, self.hidden_size)\n",
    "\t\tposition = torch.arange(0, self.max_len, dtype=torch.float).unsqueeze(1)\n",
    "\t\tdiv_term = torch.exp(torch.arange(0, self.hidden_size, 2).float() * (-log(10000.0) / self.hidden_size))\n",
    "\t\tpos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "\t\tpos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\t\tpos_enc = pos_enc.unsqueeze(0)\n",
    "\t\tself.register_buffer('pos_enc', pos_enc)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tseq_len = x.size(1)\n",
    "\t\treturn self.pos_enc[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.MHA = MultiHeadAttention(config)\n",
    "\t\tself.FFN = FeedForward(config)\n",
    "\t\tself.LN1 = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "\t\tself.LN2 = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "\t\tself.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\t\n",
    "\tdef forward(self, x, attention_mask=None):\n",
    "\t\tMHA_out = self.MHA(x, mask=attention_mask)\n",
    "\t\tx = x + self.dropout(MHA_out) #Add\n",
    "\t\tx = self.LN1(x) #And Norm\n",
    "\n",
    "\t\tFFN_out = self.FFN(x)\n",
    "\t\tx = x + self.dropout(FFN_out) #Add\n",
    "\t\tx = self.LN2(x) #And Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.masked_MHA = MultiHeadAttention(config)\n",
    "\t\tself.cross_MHA = MultiHeadAttention(config)\n",
    "\t\tself.FFN = FeedForward(config)\n",
    "\t\tself.LN1 = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "\t\tself.LN2 = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "\t\tself.LN3 = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "\t\tself.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "\tdef forward(self, x, enc_out, masked_self_attn_mask=None, cross_attn_mask=None):\n",
    "\t\tself_attn_out = self.masked_MHA(x, mask=masked_self_attn_mask)\n",
    "\t\tx = x + self.dropout(self_attn_out)\n",
    "\t\tx = self.LN1(x)\n",
    "\n",
    "\t\t#Cross-attention to get encoder-output\n",
    "\t\tcross_attn_out = self._cross_attention(x, enc_out, cross_attn_mask)\n",
    "\t\tx = x + self.dropout(cross_attn_out)\n",
    "\t\tx = self.LN2(x)\n",
    "\n",
    "\t\tFFN_out = self.FFN(x)\n",
    "\t\tx = x + self.dropout(FFN_out)\n",
    "\t\tx = self.LN3(x)\n",
    "\t\treturn x\n",
    "\t\n",
    "\tdef _cross_attention(self, dec_state, enc_state, mask=None):\n",
    "\t\tbatch, target_len, hidden = dec_state.size()\n",
    "\t\tsrc = enc_state.size(1)\n",
    "\n",
    "\t\tconfig = type('C', (), {})()\n",
    "\t\tconfig.hidden_size = hidden\n",
    "\t\tconfig.num_attention_heads = self.masked_MHA.num_heads\n",
    "\t\tembed_dim = hidden\n",
    "\t\tnum_heads = config.num_attention_heads\n",
    "\t\thead_dim = embed_dim // num_heads\n",
    "\n",
    "\t\tq_linears = nn.ModuleList([nn.Linear(embed_dim, head_dim) for _ in range(num_heads)]).to(dec_state.device)\n",
    "\t\tk_linears = nn.ModuleList([nn.Linear(embed_dim, head_dim) for _ in range(num_heads)]).to(dec_state.device)\n",
    "\t\tv_linears = nn.ModuleList([nn.Linear(embed_dim, head_dim) for _ in range(num_heads)]).to(dec_state.device)\n",
    "\t\tout_linear = nn.Linear(embed_dim, embed_dim).to(dec_state.device)\n",
    "\n",
    "\t\thead_outputs = []\n",
    "\t\tfor i in range(num_heads):\n",
    "\t\t\tq = q_linears[i](dec_state)\n",
    "\t\t\tk = k_linears[i](enc_state)\n",
    "\t\t\tv = v_linears[i](enc_state)\n",
    "\n",
    "\t\t\t#Kan vel egentlig bruke attention-funksjonen jeg lagde lenger opp?\n",
    "\t\t\tscores = torch.bmm(q, k.transpose(1,2)) / sqrt(head_dim)\n",
    "\t\t\tif mask is not None:\n",
    "\t\t\t\tscores = scores.masked_fill(mask.bool(), float('-1e9'))\n",
    "\t\t\tweights = F.softmax(scores, dim=-1)\n",
    "\t\t\thead_out = torch.bmm(weights, v)\n",
    "\t\t\thead_outputs.append(head_out)\n",
    "\t\tconcat = torch.cat(head_outputs, dim=-1)\n",
    "\t\tout = out_linear(concat)\n",
    "\t\treturn out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, config, num_layers=6):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layers = nn.ModuleList([EncoderLayer(config) for _ in range(num_layers)])\n",
    "\t\tself.ln_final = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "\t\n",
    "\tdef forward(self, x, attn_mask=None):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x, attention_mask=attn_mask)\n",
    "\t\tx = self.ln_final(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, config, num_layers=6):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layers = nn.ModuleList([DecoderLayer(config) for _ in range(num_layers)])\n",
    "\t\tself.ln_final = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "\t\n",
    "\tdef forward(self, x, enc_state, masked_self_attn_mask=None, cross_attn_mask=None):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x, enc_state, masked_self_attn_mask=masked_self_attn_mask, cross_attn_mask=cross_attn_mask)\n",
    "\t\tx = self.ln_final(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "<div>\n",
    "<img src=\"images/transformer_architecture.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "\tdef __init__(self, config, encoder_layers=6, decoder_layers=6):\n",
    "\t\tsuper().__init__()\n",
    "\t\tprint(config)\n",
    "\t\tself.config = config\n",
    "\t\tself.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\t\tself.pos_encoder = PositionalEncoding(config)\n",
    "\t\tself.encoder = Encoder(config, num_layers=encoder_layers)\n",
    "\t\tself.decoder = Decoder(config, num_layers=decoder_layers)\n",
    "\t\tself.final_linear = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\t\tself.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\t\n",
    "\tdef forward(self, input_ids, output_ids, input_mask=None, output_mask=None):\n",
    "\t\tinput_embeddings = self.token_embeddings(input_ids) + self.pos_encoder(self.token_embeddings(input_ids))\n",
    "\t\tinput_embeddings = self.dropout(input_embeddings)\n",
    "\n",
    "\t\toutput_embeddings = self.token_embeddings(output_ids) + self.pos_encoder(self.token_embeddings(output_ids))\n",
    "\t\toutput_embeddings = self.dropout(output_embeddings)\n",
    "\n",
    "\t\tencoder_attn_mask = None\n",
    "\t\tif input_mask is not None:\n",
    "\t\t\tbatch, input_len = input_ids.size()\n",
    "\t\t\tencoder_attn_mask = input_mask.unsqueeze(1).expand(batch, input_len, input_len)\n",
    "\t\t\n",
    "\t\tbatch, output_len = output_ids.size()\n",
    "\t\tcausal_mask = torch.triu(torch.ones((output_len, output_len), device=output_ids.device), diagonal=1).bool()\n",
    "\t\tmasked_self_attn_mask = causal_mask.unsqueeze(0).expand(batch, output_len, output_len)\n",
    "\n",
    "\t\tif output_mask is not None:\n",
    "\t\t\tpad_mask = output_mask.unsqueeze(1).expand(batch, output_len, output_len)\n",
    "\t\t\tmasked_self_attn_mask = masked_self_attn_mask | pad_mask.bool()\n",
    "\t\t\n",
    "\t\tcross_attn_mask = None\n",
    "\t\tif input_mask is not None:\n",
    "\t\t\tcross_attn_mask = input_mask.unsqueeze(1).expand(batch, output_len, input_len)\n",
    "\t\t\n",
    "\t\tenc_out = self.encoder(input_embeddings, attn_mask=encoder_attn_mask)\n",
    "\t\tdec_out = self.decoder(output_embeddings, enc_out, masked_self_attn_mask=masked_self_attn_mask, cross_attn_mask=cross_attn_mask)\n",
    "\n",
    "\t\tlogits = self.final_linear(dec_out)\n",
    "\t\treturn logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# --------------------------\n",
    "# 1️⃣ Tiny English–Norwegian dictionary\n",
    "# --------------------------\n",
    "eng2nor = {\n",
    "    \"i\": \"jeg\",\n",
    "    \"you\": \"du\",\n",
    "    \"he\": \"han\",\n",
    "    \"she\": \"hun\",\n",
    "    \"we\": \"vi\",\n",
    "    \"they\": \"de\",\n",
    "    \"like\": \"liker\",\n",
    "    \"eat\": \"spiser\",\n",
    "    \"play\": \"spiller\",\n",
    "    \"see\": \"ser\",\n",
    "    \"love\": \"elsker\",\n",
    "    \"dog\": \"hund\",\n",
    "    \"cat\": \"katt\",\n",
    "    \"pizza\": \"pizza\",\n",
    "    \"apple\": \"eple\",\n",
    "    \"soccer\": \"fotball\",\n",
    "    \"milk\": \"melk\",\n",
    "    \"bread\": \"brød\"\n",
    "}\n",
    "\n",
    "# generate simple sentences like “i like pizza”, “they play soccer”\n",
    "subjects = [\"i\", \"you\", \"he\", \"she\", \"we\", \"they\"]\n",
    "verbs = [\"like\", \"eat\", \"play\", \"see\", \"love\"]\n",
    "objects = [\"pizza\", \"apple\", \"soccer\", \"milk\", \"bread\", \"dog\", \"cat\"]\n",
    "\n",
    "def make_sentence():\n",
    "    s = f\"{random.choice(subjects)} {random.choice(verbs)} {random.choice(objects)}\"\n",
    "    t = \" \".join(eng2nor[w] for w in s.split())\n",
    "    return s, t\n",
    "\n",
    "pairs = [make_sentence() for _ in range(50)]\n",
    "print(\"Example pairs:\")\n",
    "for e, n in pairs[:5]:\n",
    "    print(f\"{e:20s} → {n}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2️⃣ Build vocabulary\n",
    "# --------------------------\n",
    "def build_vocab(pairs):\n",
    "    vocab = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2}\n",
    "    for eng, nor in pairs:\n",
    "        for w in (eng + \" \" + nor).split():\n",
    "            if w not in vocab:\n",
    "                vocab[w] = len(vocab)\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "vocab, inv_vocab = build_vocab(pairs)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# --------------------------\n",
    "# 3️⃣ Encode sequences\n",
    "# --------------------------\n",
    "def encode(sentence, vocab, max_len):\n",
    "    ids = [vocab[\"<s>\"]] + [vocab[w] for w in sentence.split()] + [vocab[\"</s>\"]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "max_len = max(len((e + \" \" + n).split()) + 2 for e, n in pairs)\n",
    "\n",
    "input_ids = torch.tensor([encode(e, vocab, max_len) for e, _ in pairs])\n",
    "output_ids = torch.tensor([encode(n, vocab, max_len) for _, n in pairs])\n",
    "input_mask = (input_ids == vocab[\"<pad>\"])\n",
    "output_mask = (output_ids == vocab[\"<pad>\"])\n",
    "\n",
    "# --------------------------\n",
    "# 4️⃣ Init Transformer\n",
    "# --------------------------\n",
    "cfg = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "model = SimpleTransformer(cfg, encoder_layers=2, decoder_layers=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "\n",
    "# --------------------------\n",
    "# 5️⃣ Training loop (very small)\n",
    "# --------------------------\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    decoder_input = output_ids[:, :-1]\n",
    "    target_output = output_ids[:, 1:]\n",
    "\n",
    "    logits = model(input_ids, decoder_input, input_mask, output_mask[:, :-1])\n",
    "    loss = criterion(logits.reshape(-1, vocab_size), target_output.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 6️⃣ Try your own sentence!\n",
    "# --------------------------\n",
    "def translate(sentence, model, vocab, inv_vocab, max_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = torch.tensor([encode(sentence, vocab, max_len)])\n",
    "        src_mask = (src == vocab[\"<pad>\"])\n",
    "\n",
    "        # start token for decoder\n",
    "        decoder_input = torch.tensor([[vocab[\"<s>\"]]])\n",
    "        output = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits = model(src, decoder_input, src_mask)\n",
    "            next_token = torch.argmax(logits[0, -1])\n",
    "            if next_token.item() == vocab[\"</s>\"]:\n",
    "                break\n",
    "            output.append(inv_vocab[next_token.item()])\n",
    "            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        return \" \".join(output)\n",
    "\n",
    "test_sentence = \"i like pizza\"\n",
    "print(f\"\\nInput: {test_sentence}\")\n",
    "print(\"Output:\", translate(test_sentence, model, vocab, inv_vocab, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
