{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook config\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "figsize=(14, 4)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Embedding, Masking\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "pd.set_option('display.max_columns', None)\n",
    "data_folder = os.path.join('../../..', 'data')\n",
    "file_name = \"Sample_0.01_amex_train_data.csv\"\n",
    "label_name = \"amex_train_labels.csv\"\n",
    "file_path = os.path.join(data_folder, file_name)\n",
    "label_path = os.path.join(data_folder, label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)\n",
    "labels = pd.read_csv(label_path)\n",
    "df = data.merge(labels, on=\"customer_ID\", how=\"left\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by=[\"customer_ID\", \"S_2\"])\n",
    "n = 1\n",
    "first_n_ids = df_sorted[\"customer_ID\"].unique()[:n]\n",
    "df_first_n = df_sorted[df_sorted[\"customer_ID\"].isin(first_n_ids)]\n",
    "print(df_first_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df_first_n.select_dtypes(include=\"number\")\n",
    "sns.heatmap(numeric_df.T, cmap=\"viridis\", cbar=True)\n",
    "plt.title(\"Progression of columns\")\n",
    "plt.xlabel(\"Row index\")\n",
    "plt.ylabel(\"Columns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = numeric_df.columns.tolist()\n",
    "chunk_size = 10\n",
    "for i in range(0, len(cols), chunk_size):\n",
    "\tchunk = cols[i:i+chunk_size]\n",
    "\tfor col in chunk:\n",
    "\t\tplt.plot(numeric_df.index, numeric_df[col], label=col, alpha=0.7)\n",
    "\tplt.xlabel(\"Row index\")\n",
    "\tplt.ylabel(\"Value\")\n",
    "\tplt.title(f\"Columns {i+1} to {i+len(chunk)}\")\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Vektorisering av data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted = data.sort_values(by=[\"customer_ID\", \"S_2\"])\n",
    "print(sorted.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_cols = [\"customer_ID\", \"S_2\"]\n",
    "categorical_cols = [\"D_63\", \"D_64\"]\n",
    "feature_cols = [col for col in sorted.columns if col not in sequence_cols]\n",
    "numerical_cols = [col for col in feature_cols if col not in categorical_cols]\n",
    "\n",
    "sorted[numerical_cols] = sorted.groupby(\"customer_ID\")[numerical_cols].ffill().fillna(0)\n",
    "\n",
    "for col in categorical_cols:\n",
    "\tle = LabelEncoder()\n",
    "\tsorted[col] = le.fit_transform(sorted[col])\n",
    "\n",
    "customer_sequence = {}\n",
    "for customer_id, group in sorted.groupby(\"customer_ID\"):\n",
    "\tcustomer_vector_sequence = group[feature_cols].values\n",
    "\tcustomer_sequence[customer_id] = customer_vector_sequence\n",
    "\n",
    "customer_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = dict(zip(labels[\"customer_ID\"], labels[\"target\"]))\n",
    "target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialTransactionClassifier:\n",
    "    def __init__(self, max_sequence_length=100, model_type='lstm'):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_data(self, transaction_data, target_data):\n",
    "        \"\"\"\n",
    "        Prepare sequential data for training\n",
    "        \n",
    "        Args:\n",
    "            transaction_data: dict {customer_id: [list of transaction vectors]}\n",
    "            target_data: dict {customer_id: 0 or 1}\n",
    "        \"\"\"\n",
    "        # Get common customer IDs\n",
    "        common_ids = list(set(transaction_data.keys()) & set(target_data.keys()))\n",
    "        \n",
    "        # Extract sequences and targets\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        \n",
    "        for cid in common_ids:\n",
    "            sequence = transaction_data[cid]\n",
    "            if len(sequence) > 0:  # Only include customers with transactions\n",
    "                sequences.append(sequence)\n",
    "                targets.append(target_data[cid])\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        # Handle different sequence lengths with padding\n",
    "        if len(sequences[0]) > 0 and isinstance(sequences[0][0], (list, np.ndarray)):\n",
    "            # Multi-dimensional transaction vectors\n",
    "            max_len = min(max(len(seq) for seq in sequences), self.max_sequence_length)\n",
    "            \n",
    "            # Pad sequences\n",
    "            padded_sequences = []\n",
    "            for seq in sequences:\n",
    "                if len(seq) > max_len:\n",
    "                    padded_seq = seq[-max_len:]  # Take last max_len transactions\n",
    "                else:\n",
    "                    # Pad with zeros\n",
    "                    pad_length = max_len - len(seq)\n",
    "                    padded_seq = [[0] * len(seq[0])] * pad_length + seq\n",
    "                padded_sequences.append(padded_seq)\n",
    "            \n",
    "            sequences = np.array(padded_sequences)\n",
    "            \n",
    "        else:\n",
    "            # 1D transaction values\n",
    "            sequences = pad_sequences(sequences, maxlen=self.max_sequence_length, \n",
    "                                    dtype='float32', padding='pre', truncating='pre')\n",
    "            # Reshape for LSTM input (samples, timesteps, features)\n",
    "            sequences = sequences.reshape(sequences.shape[0], sequences.shape[1], 1)\n",
    "        \n",
    "        return sequences, targets\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"Build the sequential model\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Masking layer to handle padded zeros\n",
    "        model.add(Masking(mask_value=0.0, input_shape=input_shape))\n",
    "        \n",
    "        if self.model_type == 'lstm':\n",
    "            # LSTM layers\n",
    "            model.add(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "        elif self.model_type == 'gru':\n",
    "            # GRU layers\n",
    "            model.add(GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model.add(GRU(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "        elif self.model_type == 'bidirectional':\n",
    "            # Bidirectional LSTM\n",
    "            from tensorflow.keras.layers import Bidirectional\n",
    "            model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.2)))\n",
    "            model.add(Bidirectional(LSTM(32, dropout=0.2)))\n",
    "        \n",
    "        # Dense layers\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, transaction_data, target_data, validation_split=0.2, epochs=50, batch_size=32):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Prepare data\n",
    "        X, y = self.prepare_data(transaction_data, target_data)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Build model\n",
    "        input_shape = (X.shape[1], X.shape[2]) if len(X.shape) == 3 else (X.shape[1], 1)\n",
    "        self.model = self.build_model(input_shape)\n",
    "        \n",
    "        print(\"Model Architecture:\")\n",
    "        self.model.summary()\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc, test_prec, test_rec = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Test Precision: {test_prec:.4f}\")\n",
    "        print(f\"Test Recall: {test_rec:.4f}\")\n",
    "        \n",
    "        # Predictions and classification report\n",
    "        y_pred_proba = self.model.predict(X_test)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).reshape(-1)\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        return history, X_test, y_test, y_pred\n",
    "    \n",
    "    def predict(self, transaction_sequences):\n",
    "        \"\"\"Make predictions on new transaction sequences\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model hasn't been trained yet!\")\n",
    "        \n",
    "        # Prepare sequences (similar to training data preparation)\n",
    "        if isinstance(transaction_sequences, dict):\n",
    "            sequences = list(transaction_sequences.values())\n",
    "        else:\n",
    "            sequences = transaction_sequences\n",
    "        \n",
    "        # Pad sequences to match training format\n",
    "        if len(sequences[0]) > 0 and isinstance(sequences[0][0], (list, np.ndarray)):\n",
    "            # Multi-dimensional\n",
    "            padded_sequences = []\n",
    "            for seq in sequences:\n",
    "                if len(seq) > self.max_sequence_length:\n",
    "                    padded_seq = seq[-self.max_sequence_length:]\n",
    "                else:\n",
    "                    pad_length = self.max_sequence_length - len(seq)\n",
    "                    padded_seq = [[0] * len(seq[0])] * pad_length + seq\n",
    "                padded_sequences.append(padded_seq)\n",
    "            sequences = np.array(padded_sequences)\n",
    "        else:\n",
    "            # 1D\n",
    "            sequences = pad_sequences(sequences, maxlen=self.max_sequence_length, \n",
    "                                    dtype='float32', padding='pre', truncating='pre')\n",
    "            sequences = sequences.reshape(sequences.shape[0], sequences.shape[1], 1)\n",
    "        \n",
    "        predictions = self.model.predict(sequences)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SequentialTransactionClassifier(model_type=\"lstm\")\n",
    "history, X_test, y_test, y_pred = classifier.train(customer_sequence, target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
