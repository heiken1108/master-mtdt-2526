{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "figsize=(14, 4)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import file, plot, data, stat\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, ConfusionMatrixDisplay, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "pd.set_option('display.max_columns', None)\n",
    "data_folder = os.path.join('../../../..', 'data/prod')\n",
    "file_name_collection = \"Collection_data.csv\"\n",
    "file_path_collection = os.path.join(data_folder, file_name_collection)\n",
    "file_name_konto = \"konto_data_trimmed.csv\"\n",
    "file_path_konto = os.path.join(data_folder, file_name_konto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "konto_frame, collection_frame = file.load_konto_data(\n",
    "    file_path_konto\n",
    "), file.load_collection_data(file_path_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_in_kont = konto_frame[\"PersonId\"].unique()\n",
    "collection_frame = collection_frame[collection_frame[\"PersonId\"].isin(ids_in_kont)]\n",
    "target_frame = (\n",
    "    collection_frame[\n",
    "        [\n",
    "            \"Collectionid\",\n",
    "            \"PersonId\",\n",
    "            \"AccountId\",\n",
    "            \"CollectionOpenedDate\",\n",
    "            \"CollectionClosedDate\",\n",
    "            \"BalanceSentAmt\",\n",
    "            \"MonthInDCA\",\n",
    "            \"CumulativeLossAmt\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby(\"Collectionid\")\n",
    "    .tail(1)\n",
    "    .set_index(\"Collectionid\")\n",
    ")\n",
    "# Stenger saker når de går til overvåk\n",
    "zcov_frame = collection_frame[collection_frame[\"MonthsInZCOV\"] == 0].copy()\n",
    "zcov_frame = zcov_frame.sort_values(\"YearMonth\").drop_duplicates(\"Collectionid\")\n",
    "zcov_frame[\"ZCOVDate\"] = zcov_frame[\"YearMonth\"] + pd.offsets.MonthEnd(0)\n",
    "target_frame[\"ZCOVDate\"] = target_frame.index.map(\n",
    "    zcov_frame.set_index(\"Collectionid\")[\"ZCOVDate\"]\n",
    ")\n",
    "target_frame[\"ClosedDateSetByZCOVDate\"] = 0\n",
    "cond = target_frame[\"CollectionClosedDate\"].isna() & target_frame[\"ZCOVDate\"].notna()\n",
    "target_frame.loc[cond, \"CollectionClosedDate\"] = target_frame.loc[cond, \"ZCOVDate\"]\n",
    "target_frame.loc[cond, \"ClosedDateSetByZCOVDate\"] = 1\n",
    "\n",
    "# Flagger om sak fortsatt er åpen\n",
    "target_frame[\"IsOpen\"] = (target_frame[\"CollectionClosedDate\"].isna()).astype(int)\n",
    "\n",
    "# Flagger om sak er åpnet innenfor datasettet\n",
    "target_frame[\"CollectionOpenedAfter202309\"] = (\n",
    "    target_frame[\"CollectionOpenedDate\"] >= pd.Timestamp(\"2023-09-01\")\n",
    ").astype(int)\n",
    "\n",
    "# Beregner antall dager saken har vært åpen. Merk at ZCOVDate er satt til siste dag i måneden\n",
    "target_frame[\"DurationDays\"] = (\n",
    "    pd.to_datetime(target_frame[\"CollectionClosedDate\"])\n",
    "    - pd.to_datetime(target_frame[\"CollectionOpenedDate\"])\n",
    ").dt.days\n",
    "\n",
    "# Definerer tap. Tap under 0 settes til 0. Tap over balanse sent settes til balanse\n",
    "target_frame[\"Loss\"] = (\n",
    "    target_frame[\"CumulativeLossAmt\"]\n",
    "    .mask(target_frame[\"CumulativeLossAmt\"] < 0, 0)\n",
    "    .mask(\n",
    "        target_frame[\"CumulativeLossAmt\"] > target_frame[\"BalanceSentAmt\"],\n",
    "        target_frame[\"BalanceSentAmt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Beregner LGD som en andel av balanse\n",
    "target_frame[\"LGD\"] = target_frame[\"Loss\"] / target_frame[\"BalanceSentAmt\"]\n",
    "\n",
    "# Fjerner alle saker som enten er uten balanse, er åpnet før datasettet, eller ikke er stengt\n",
    "mask_filter = (\n",
    "    (target_frame[\"BalanceSentAmt\"] != 0)\n",
    "    & (target_frame[\"CollectionOpenedAfter202309\"] == 1)\n",
    "    & (target_frame[\"IsOpen\"] == 0)\n",
    "    & (target_frame[\"LGD\"] > 0)\n",
    "    & (target_frame[\"LGD\"] < 1)\n",
    "\t\n",
    "\t\n",
    ")\n",
    "target_frame = target_frame[mask_filter]\n",
    "\n",
    "\n",
    "# Kolonne man gjør klassifisering på\n",
    "target_col = \"LGD\"\n",
    "target_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 25\n",
    "\n",
    "# Create bins (0 to 1)\n",
    "target_frame[\"binned_target\"] = pd.cut(\n",
    "    target_frame[target_col], bins=num_bins, include_lowest=True\n",
    ")\n",
    "\n",
    "# Count values in each bin\n",
    "bin_counts = target_frame[\"binned_target\"].value_counts().sort_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=bin_counts.index.astype(str), y=bin_counts.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(f\"Binned {target_col} (intervals between 0 and 1)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Distribution of {target_col} binned into {num_bins} intervals\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for idx, row in target_frame.iterrows():\n",
    "    mask = (\n",
    "        (konto_frame[\"PersonId\"] == row[\"PersonId\"])\n",
    "        & (konto_frame[\"AccountId\"] == row[\"AccountId\"])\n",
    "        & (konto_frame[\"YearMonth\"] < row[\"CollectionOpenedDate\"])\n",
    "    )\n",
    "    temp = konto_frame[mask].copy()\n",
    "    temp[\"Collectionid\"] = row.name\n",
    "    rows.append(temp)\n",
    "feature_frame = pd.concat(rows, ignore_index=True)\n",
    "feature_frame = data.reorder_column(feature_frame, \"Collectionid\", 3)\n",
    "feature_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_map = {\n",
    "    \"Normal\": 0,\n",
    "    \"Ikke aktivert\": 0,\n",
    "    \"Avsluttet\": 0,\n",
    "    \"Avsluttet av kunde\": 0,\n",
    "    #\n",
    "    \"Faktura forfalt\": 1,\n",
    "    \"Purring\": 1,\n",
    "    \"Under avslutning\": 1,\n",
    "    \"Under avslutning manglende KYC\": 1,\n",
    "    \"Sperret\": 1,\n",
    "    \"Sperret, propagert\": 1,\n",
    "    \"Eget misbruk\": 1,\n",
    "    \"Avsluttet inkasso, oppgjort\": 1,\n",
    "    #\n",
    "    \"Purring forfalt\": 2,\n",
    "    \"Purring med kortsperre\": 2,\n",
    "    \"Spesialengasjement, manuell behandling\": 2,\n",
    "    \"Betalingsplan, nedbetaling\": 2,\n",
    "    #\n",
    "    \"Inkassovarsel\": 3,\n",
    "    #\n",
    "    \"Overført inkassobyrå\": 4,\n",
    "    #\n",
    "    \"Inkasso, overført overvåk\": 5,\n",
    "    \"Avsluttet inkasso, med tap\": 5,\n",
    "    \"Gjeldsordning ikke DCA Offentlig\": 5,\n",
    "    \"Gjeldsordning ikke DCA Privat\": 5,\n",
    "    \"Konkursbo Privat\": 5,\n",
    "}\n",
    "feature_frame[\"RisikoStatus\"] = feature_frame[\"GeneralStatusDesc\"].map(status_map)\n",
    "feature_frame = data.reorder_column(feature_frame, \"RisikoStatus\", 10)\n",
    "product_map = {\n",
    "    1: \"SB1 GOLD MC\",\n",
    "    2: \"Sparebank 1 Platinum MC\",\n",
    "    4: \"SH BUSINESS VISA\",\n",
    "    7: \"LOfavør Mastercard\",\n",
    "    8: \"SB1 UNG MC\",\n",
    "    30: \"SH GOLD MC\",\n",
    "    34: \"SB1 EXTRA MC\",\n",
    "    38: \"LOfavør Mastercard Ung\",\n",
    "    40: \"SpareBank 1 Mastercard Elite\",\n",
    "}\n",
    "feature_frame[\"ProductId\"] = feature_frame[\"ProductId\"].map(product_map)\n",
    "feature_frame.rename(columns={\"ProductId\": \"Productname\"}, inplace=True)\n",
    "feature_frame[\"Gender\"] = feature_frame[\"Gender\"].map({\"F\": 0, \"M\": 1}).astype(int)\n",
    "feature_frame[\"AgeGroup\"] = (\n",
    "    feature_frame[\"AgeGroup\"]\n",
    "    .map(\n",
    "        {\"0 - 24\": 0, \"25 - 34\": 1, \"35 - 44\": 2, \"45 - 54\": 3, \"55 - 64\": 4, \"> 64\": 5}\n",
    "    )\n",
    "    .astype(int)\n",
    ")\n",
    "col_rename_dict = {\"Gender\": \"Male\"}\n",
    "feature_frame.rename(columns=col_rename_dict, inplace=True)\n",
    "feature_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = {\n",
    "    \"PersonId\": \"last\",\n",
    "    \"Productname\": \"last\",\n",
    "    \"DistributorId\": \"last\",\n",
    "    \"AgeGroup\": \"last\",\n",
    "    \"Male\": \"last\",\n",
    "    \"MonthsSinceAccountCreatedNum\": \"last\",\n",
    "    \"BalanceAmt\": \"last\",\n",
    "    \"CreditLimitAmt\": \"last\",\n",
    "    \"OverdueAmt\": \"last\",\n",
    "    \"StatementClosingBalanceAmt\": \"last\",\n",
    "    \"Last_Airlines\": \"sum\",\n",
    "    \"Last_Amusement and Entertainment\": \"sum\",\n",
    "    \"Last_Automobile / Vehicle Rental\": \"sum\",\n",
    "    \"Last_Business Services\": \"sum\",\n",
    "    \"Last_Clothing Stores\": \"sum\",\n",
    "    \"Last_Contracted Services\": \"sum\",\n",
    "    \"Last_Government Services\": \"sum\",\n",
    "    \"Last_Hotels\": \"sum\",\n",
    "    \"Last_Includes all lodging merchants\": \"sum\",\n",
    "    \"Last_Mail Order / Telephone Order Providers\": \"sum\",\n",
    "    \"Last_Miscellaneous Stores\": \"sum\",\n",
    "    \"Last_Others\": \"sum\",\n",
    "    \"Last_Professional Services and Membership Organizations\": \"sum\",\n",
    "    \"Last_Repair Services\": \"sum\",\n",
    "    \"Last_Retail Stores\": \"sum\",\n",
    "    \"Last_Service Providers\": \"sum\",\n",
    "    \"Last_Transportation\": \"sum\",\n",
    "    \"Last_Utilities\": \"sum\",\n",
    "    \"Last_Wholesale Distributors and Manufacturers\": \"sum\",\n",
    "}\n",
    "feature_copy = feature_frame.copy()\n",
    "agg_frame = (\n",
    "    feature_copy.groupby(\"Collectionid\")\n",
    "    .agg(agg_dict)\n",
    "    .reset_index()\n",
    "    .set_index(\"Collectionid\")\n",
    ")\n",
    "# Antall måneder med data\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"YearMonth\"].nunique().rename(\"NumberOfMonths\")\n",
    ")\n",
    "# Antall kontoer\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"AccountId\"]\n",
    "    .nunique()\n",
    "    .rename(\"NumberOfAccounts\")\n",
    ")\n",
    "# Snitt av statusene\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"RisikoStatus\"]\n",
    "    .mean()\n",
    "    .rename(\"RisikoStatusSnitt\")\n",
    ")\n",
    "# Sum av endringer fra måned til måned\n",
    "agg_frame = agg_frame.join(  # Det må håndteres at en person kan ha ulike kontoer. Ha et merge accounts steg\n",
    "    feature_copy.groupby(\"Collectionid\")[\"RisikoStatus\"]\n",
    "    .apply(lambda x: x.diff().abs().sum())\n",
    "    .rename(\"SumRisikoStatusEndringer\")\n",
    ")\n",
    "# Snitt endringer i måneden\n",
    "agg_frame[\"SnittRisikoStatusEndringerPerMåned\"] = (\n",
    "    agg_frame[\"SumRisikoStatusEndringer\"] / agg_frame[\"NumberOfMonths\"]\n",
    ")\n",
    "# Andel av hver risikogruppe\n",
    "risk_counts = (\n",
    "    feature_copy.groupby(\"Collectionid\")[\"RisikoStatus\"]\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "risk_props = risk_counts.div(risk_counts.sum(axis=1), axis=0)\n",
    "risk_props.columns = [f\"RisikoStatus_{col}_share\" for col in risk_props.columns]\n",
    "agg_frame = agg_frame.join(risk_props)\n",
    "# Snitt-balanse\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"BalanceAmt\"].mean().rename(\"BalanceAmtMean\")\n",
    ")\n",
    "# Standardavvik balanse\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"BalanceAmt\"]\n",
    "    .std()\n",
    "    .fillna(0)\n",
    "    .rename(\"BalanceAmtStd\")\n",
    ")\n",
    "\n",
    "# Antall kredittgrenseøkninger\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"CreditLimitIncreaseFlag\"]\n",
    "    .sum()\n",
    "    .rename(\"AntallCreditLimitIncrease\")\n",
    ")\n",
    "# Antall ganger gått til inkasso\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"CollectionFlag\"]\n",
    "    .sum()\n",
    "    .rename(\"AntallCollectionFlag\")\n",
    ")\n",
    "# Snitt ganger gått til inkasso\n",
    "agg_frame[\"SnittCollectionFlagPerMåned\"] = (\n",
    "    agg_frame[\"AntallCollectionFlag\"] / agg_frame[\"NumberOfMonths\"]\n",
    ")\n",
    "# Sum turnover\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"TurnoverAmt\"].sum().rename(\"SumTurnover\")\n",
    ")\n",
    "# Snitt turnover\n",
    "agg_frame[\"SnittTurnover\"] = agg_frame[\"SumTurnover\"] / agg_frame[\"NumberOfMonths\"]\n",
    "# Sum antall transaksjoner\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"TurnoverNum\"].sum().rename(\"TurnoverNumSum\")\n",
    ")\n",
    "# Snitt antall transaksjoner\n",
    "agg_frame[\"SnittTurnoverNum\"] = (\n",
    "    agg_frame[\"TurnoverNumSum\"] / agg_frame[\"NumberOfMonths\"]\n",
    ")\n",
    "# Snitt transaksjonsstørrelse\n",
    "agg_frame[\"AverageTransactionSize\"] = (\n",
    "    agg_frame[\"SumTurnover\"] / agg_frame[\"TurnoverNumSum\"]\n",
    ").fillna(0)\n",
    "# Sum innenlandstransaksjoner\n",
    "agg_frame = agg_frame.join(  # X\n",
    "    feature_copy.groupby(\"Collectionid\")[\"TurnoverDomAmt\"]\n",
    "    .sum()\n",
    "    .rename(\"SumTurnoverDom\")\n",
    ")\n",
    "agg_frame[\"ShareOfTurnoverIsDomestic\"] = (\n",
    "    agg_frame[\"SumTurnoverDom\"] / agg_frame[\"SumTurnover\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Withdrawal sum\n",
    "agg_frame = agg_frame.join(  # X\n",
    "    feature_copy.groupby(\"Collectionid\")[\"FundtransferAmt\"]\n",
    "    .sum()\n",
    "    .rename(\"SumFundtransfer\")\n",
    ")\n",
    "agg_frame = agg_frame.join(  # X\n",
    "    feature_copy.groupby(\"Collectionid\")[\"CashAtmAmt\"].sum().rename(\"SumCashAtm\")\n",
    ")\n",
    "agg_frame = agg_frame.join(  # X\n",
    "    feature_copy.groupby(\"Collectionid\")[\"CashCounterAmt\"]\n",
    "    .sum()\n",
    "    .rename(\"SumCashCounter\")\n",
    ")\n",
    "agg_frame[\"WithdrawalSum\"] = (\n",
    "    agg_frame[\"SumFundtransfer\"] + agg_frame[\"SumCashAtm\"] + agg_frame[\"SumCashCounter\"]\n",
    ")\n",
    "\n",
    "# Withdrawal propotional to turnover\n",
    "agg_frame[\"WithdrawalPropOfTurnover\"] = (\n",
    "    agg_frame[\"WithdrawalSum\"] / agg_frame[\"SumTurnover\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Total expediture as propotion of kredittgrense\n",
    "agg_frame[\"ExpenditureAsShareOfCreditlimit\"] = (\n",
    "    agg_frame[\"SumTurnover\"] + agg_frame[\"WithdrawalSum\"]\n",
    ") / agg_frame[\"CreditLimitAmt\"]\n",
    "\n",
    "\n",
    "# Snitt Turnover andel av kredittgrense\n",
    "feature_copy[\"AndelTurnoverAvKredittgrense\"] = (\n",
    "    feature_copy[\"TurnoverAmt\"] / feature_copy[\"CreditLimitAmt\"] * -1\n",
    ")\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"AndelTurnoverAvKredittgrense\"]\n",
    "    .mean()\n",
    "    .rename(\"SnittTurnoverAndelAvKredittgrense\")\n",
    ")\n",
    "# Siste Balanse som andel av kredittgrense\n",
    "feature_copy[\"AndelBalanseAvKredittgrense\"] = (\n",
    "    feature_copy[\"BalanceAmt\"] / feature_copy[\"CreditLimitAmt\"] * -1\n",
    ")\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"AndelBalanseAvKredittgrense\"]\n",
    "    .last()\n",
    "    .rename(\"LastAndelBalanseAvKredittgrense\")\n",
    ")\n",
    "# Antall overtrekk\n",
    "feature_copy[\"Overtrekk\"] = (\n",
    "    feature_copy[\"InterestEarningLendingAmt\"] > feature_copy[\"CreditLimitAmt\"]\n",
    ").astype(int)\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"Overtrekk\"].sum().rename(\"AntallOvertrekk\")\n",
    ")\n",
    "# Snitt overtrekk\n",
    "agg_frame[\"SnittOvertrekkPerMåned\"] = (\n",
    "    agg_frame[\"AntallOvertrekk\"] / agg_frame[\"NumberOfMonths\"]\n",
    ")\n",
    "\n",
    "# Andel av overdue kontra størrelse på faktura\n",
    "agg_frame[\"OverdueShareOfDebt\"] = (\n",
    "    agg_frame[\"OverdueAmt\"] / agg_frame[\"StatementClosingBalanceAmt\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Snitt antall payment overdue\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"PaymentOverDueFlag\"]\n",
    "    .mean()\n",
    "    .rename(\"SnittPaymentOverDueFlag\")\n",
    ")\n",
    "# Snitt Revolvingflag\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"RevolvingFlag\"]\n",
    "    .mean()\n",
    "    .rename(\"SnittRevolvingFlag\")\n",
    ")\n",
    "\n",
    "cols_to_share_by_turnover = [\n",
    "    \"Last_Airlines\",\n",
    "    \"Last_Amusement and Entertainment\",\n",
    "    \"Last_Automobile / Vehicle Rental\",\n",
    "    \"Last_Business Services\",\n",
    "    \"Last_Clothing Stores\",\n",
    "    \"Last_Contracted Services\",\n",
    "    \"Last_Government Services\",\n",
    "    \"Last_Hotels\",\n",
    "    \"Last_Includes all lodging merchants\",\n",
    "    \"Last_Mail Order / Telephone Order Providers\",\n",
    "    \"Last_Miscellaneous Stores\",\n",
    "    \"Last_Others\",\n",
    "    \"Last_Professional Services and Membership Organizations\",\n",
    "    \"Last_Repair Services\",\n",
    "    \"Last_Retail Stores\",\n",
    "    \"Last_Service Providers\",\n",
    "    \"Last_Transportation\",\n",
    "    \"Last_Utilities\",\n",
    "    \"Last_Wholesale Distributors and Manufacturers\",\n",
    "]\n",
    "for col in cols_to_share_by_turnover:\n",
    "    string = re.sub(r\"^Last_\", \"\", col)\n",
    "    string = f\"{string}_share_of_Turnover\"\n",
    "    agg_frame[string] = (agg_frame[col] / agg_frame[\"SumTurnover\"]).fillna(0)\n",
    "\n",
    "\n",
    "cols_to_drop = [\n",
    "    \"SumTurnoverDom\",\n",
    "    \"SumFundtransfer\",\n",
    "    \"SumCashAtm\",\n",
    "    \"SumCashCounter\",\n",
    "] + cols_to_share_by_turnover\n",
    "agg_frame = agg_frame.drop(cols_to_drop, axis=1)\n",
    "agg_frame_encoded = pd.get_dummies(\n",
    "    agg_frame, columns=[\"Productname\", \"DistributorId\"]\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_target_frame = agg_frame_encoded.merge(\n",
    "    target_frame.reset_index()[[\"Collectionid\", target_col]],\n",
    "    on=\"Collectionid\",\n",
    "    how=\"left\",\n",
    ")\n",
    "feature_target_frame = feature_target_frame.set_index(\"Collectionid\", drop=True)  # ?\n",
    "feature_target_frame.sort_values(by=\"PersonId\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"PersonId\", target_col]\n",
    "X = feature_target_frame.drop(columns=cols_to_drop)\n",
    "X = X.apply(lambda x: x.astype(int) if x.dtype == \"bool\" else x)\n",
    "y = feature_target_frame[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_evaluation_regression(\n",
    "    model, X_test, y_test, X_train=None, top_n_features=10, model_name=\"Model\"\n",
    "):\n",
    "    # --- Predict ---\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    # --- Feature importances ---\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        importances = (\n",
    "            np.abs(model.coef_[0])\n",
    "            if len(model.coef_.shape) > 1\n",
    "            else np.abs(model.coef_)\n",
    "        )\n",
    "    else:\n",
    "        importances = None\n",
    "\n",
    "    if importances is not None and X_train is not None:\n",
    "        feat_importance_df = (\n",
    "            pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": importances})\n",
    "            .sort_values(by=\"Importance\", ascending=False)\n",
    "            .head(top_n_features)\n",
    "        )\n",
    "    else:\n",
    "        feat_importance_df = None\n",
    "\n",
    "    # --- Plot subplots ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    # 1️⃣ Predicted vs Actual\n",
    "    sns.scatterplot(x=y_test, y=y_pred, ax=axes[0])\n",
    "    axes[0].plot([0, 1], [0, 1], \"r--\")  # Reference line y=x\n",
    "    axes[0].set_xlabel(\"Actual\")\n",
    "    axes[0].set_ylabel(\"Predicted\")\n",
    "    axes[0].set_title(f\"{model_name} - Predicted vs Actual\")\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "\n",
    "    # 2️⃣ Residuals plot\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=axes[1])\n",
    "    axes[1].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"Residuals (Actual - Predicted)\")\n",
    "    axes[1].set_title(f\"{model_name} - Residuals Plot\")\n",
    "\n",
    "    # 3️⃣ Feature importances\n",
    "    if feat_importance_df is not None:\n",
    "        axes[2].barh(\n",
    "            feat_importance_df[\"Feature\"],\n",
    "            feat_importance_df[\"Importance\"],\n",
    "            color=\"skyblue\",\n",
    "        )\n",
    "        axes[2].invert_yaxis()\n",
    "        axes[2].set_xlabel(\"Importance\")\n",
    "        axes[2].set_title(f\"{model_name} - Top {top_n_features} Features\")\n",
    "    else:\n",
    "        axes[2].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No feature importances\",\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        axes[2].set_axis_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming you already have:\n",
    "# X_train, X_test, y_train, y_test from your train_test_split\n",
    "\n",
    "\n",
    "# STEP 1: Prepare target variable for beta regression\n",
    "# Beta regression requires values strictly in (0, 1), not at boundaries\n",
    "def prepare_beta_target(y, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Ensure target variable is strictly between 0 and 1.\n",
    "\n",
    "    Parameters:\n",
    "    - y: target variable (should already be proportions/rates)\n",
    "    - epsilon: small value to clip boundaries\n",
    "    \"\"\"\n",
    "    y_clipped = y.clip(epsilon, 1 - epsilon)\n",
    "\n",
    "    # Check if any transformations were needed\n",
    "    n_zeros = (y == 0).sum()\n",
    "    n_ones = (y == 1).sum()\n",
    "\n",
    "    if n_zeros > 0 or n_ones > 0:\n",
    "        print(f\"Warning: Found {n_zeros} zeros and {n_ones} ones in target.\")\n",
    "        print(f\"Clipped to range [{epsilon}, {1-epsilon}]\")\n",
    "\n",
    "    print(f\"\\nTarget variable range: [{y_clipped.min():.6f}, {y_clipped.max():.6f}]\")\n",
    "\n",
    "    return y_clipped\n",
    "\n",
    "\n",
    "# Prepare the target variable\n",
    "y_train_beta = prepare_beta_target(y_train)\n",
    "y_test_beta = prepare_beta_target(y_test)\n",
    "\n",
    "# STEP 2: Add constant (intercept) to features\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Feature Information\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"\\nFeatures: {list(X_train.columns)}\")\n",
    "\n",
    "# STEP 3: Fit Beta Regression Model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Fitting Beta Regression Model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Using GLM with Binomial family and logit link (standard for beta regression)\n",
    "model = sm.GLM(\n",
    "    y_train_beta,\n",
    "    X_train_const,\n",
    "    family=sm.families.Binomial(link=sm.families.links.Logit()),\n",
    ")\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "print(results.summary())\n",
    "\n",
    "# STEP 4: Make Predictions\n",
    "y_pred_train = results.predict(X_train_const)\n",
    "y_pred_test = results.predict(X_test_const)\n",
    "\n",
    "\n",
    "# STEP 5: Calculate Performance Metrics\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    \"\"\"Calculate and display performance metrics.\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    # Additional metrics for proportions\n",
    "    median_ae = np.median(np.abs(y_true - y_pred))\n",
    "    max_error = np.max(np.abs(y_true - y_pred))\n",
    "\n",
    "    print(f\"\\n{dataset_name} Performance:\")\n",
    "    print(f\"  RMSE:        {rmse:.6f}\")\n",
    "    print(f\"  MAE:         {mae:.6f}\")\n",
    "    print(f\"  Median AE:   {median_ae:.6f}\")\n",
    "    print(f\"  Max Error:   {max_error:.6f}\")\n",
    "    print(f\"  R²:          {r2:.6f}\")\n",
    "    print(f\"  MAPE:        {mape:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R²\": r2,\n",
    "        \"MAPE\": mape,\n",
    "        \"Median_AE\": median_ae,\n",
    "        \"Max_Error\": max_error,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_metrics = calculate_metrics(y_train_beta, y_pred_train, \"Training Set\")\n",
    "test_metrics = calculate_metrics(y_test_beta, y_pred_test, \"Test Set\")\n",
    "\n",
    "# Check for overfitting\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Overfitting Check\")\n",
    "print(\"=\" * 70)\n",
    "r2_diff = train_metrics[\"R²\"] - test_metrics[\"R²\"]\n",
    "mae_diff = test_metrics[\"MAE\"] - train_metrics[\"MAE\"]\n",
    "\n",
    "print(f\"R² difference (train - test): {r2_diff:.6f}\")\n",
    "print(f\"MAE difference (test - train): {mae_diff:.6f}\")\n",
    "\n",
    "if r2_diff > 0.1:\n",
    "    print(\"⚠️  Warning: Possible overfitting detected (R² gap > 0.1)\")\n",
    "elif r2_diff < 0:\n",
    "    print(\"✓  No overfitting detected\")\n",
    "else:\n",
    "    print(\"✓  Minimal overfitting\")\n",
    "\n",
    "# STEP 6: Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Actual vs Predicted - Train\n",
    "axes[0, 0].scatter(\n",
    "    y_train_beta, y_pred_train, alpha=0.5, s=30, edgecolors=\"k\", linewidth=0.5\n",
    ")\n",
    "axes[0, 0].plot([0, 1], [0, 1], \"r--\", lw=2, label=\"Perfect prediction\")\n",
    "axes[0, 0].set_xlabel(\"Actual Values\", fontsize=11)\n",
    "axes[0, 0].set_ylabel(\"Predicted Values\", fontsize=11)\n",
    "axes[0, 0].set_title(\n",
    "    f'Training: Actual vs Predicted\\nR² = {train_metrics[\"R²\"]:.4f}', fontsize=12\n",
    ")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Actual vs Predicted - Test\n",
    "axes[0, 1].scatter(\n",
    "    y_test_beta,\n",
    "    y_pred_test,\n",
    "    alpha=0.5,\n",
    "    s=30,\n",
    "    color=\"orange\",\n",
    "    edgecolors=\"k\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "axes[0, 1].plot([0, 1], [0, 1], \"r--\", lw=2, label=\"Perfect prediction\")\n",
    "axes[0, 1].set_xlabel(\"Actual Values\", fontsize=11)\n",
    "axes[0, 1].set_ylabel(\"Predicted Values\", fontsize=11)\n",
    "axes[0, 1].set_title(\n",
    "    f'Test: Actual vs Predicted\\nR² = {test_metrics[\"R²\"]:.4f}', fontsize=12\n",
    ")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals - Train\n",
    "residuals_train = y_train_beta - y_pred_train\n",
    "axes[0, 2].scatter(\n",
    "    y_pred_train, residuals_train, alpha=0.5, s=30, edgecolors=\"k\", linewidth=0.5\n",
    ")\n",
    "axes[0, 2].axhline(y=0, color=\"r\", linestyle=\"--\", lw=2)\n",
    "axes[0, 2].set_xlabel(\"Predicted Values\", fontsize=11)\n",
    "axes[0, 2].set_ylabel(\"Residuals\", fontsize=11)\n",
    "axes[0, 2].set_title(\"Training: Residual Plot\", fontsize=12)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals - Test\n",
    "residuals_test = y_test_beta - y_pred_test\n",
    "axes[1, 0].scatter(\n",
    "    y_pred_test,\n",
    "    residuals_test,\n",
    "    alpha=0.5,\n",
    "    s=30,\n",
    "    color=\"orange\",\n",
    "    edgecolors=\"k\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "axes[1, 0].axhline(y=0, color=\"r\", linestyle=\"--\", lw=2)\n",
    "axes[1, 0].set_xlabel(\"Predicted Values\", fontsize=11)\n",
    "axes[1, 0].set_ylabel(\"Residuals\", fontsize=11)\n",
    "axes[1, 0].set_title(\"Test: Residual Plot\", fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Distribution comparison - Train\n",
    "axes[1, 1].hist(\n",
    "    y_train_beta, bins=30, alpha=0.6, label=\"Actual\", color=\"blue\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 1].hist(\n",
    "    y_pred_train, bins=30, alpha=0.6, label=\"Predicted\", color=\"red\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 1].set_xlabel(\"Value\", fontsize=11)\n",
    "axes[1, 1].set_ylabel(\"Frequency\", fontsize=11)\n",
    "axes[1, 1].set_title(\"Training: Distribution Comparison\", fontsize=12)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# 6. Distribution comparison - Test\n",
    "axes[1, 2].hist(\n",
    "    y_test_beta, bins=30, alpha=0.6, label=\"Actual\", color=\"blue\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 2].hist(\n",
    "    y_pred_test,\n",
    "    bins=30,\n",
    "    alpha=0.6,\n",
    "    label=\"Predicted\",\n",
    "    color=\"orange\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[1, 2].set_xlabel(\"Value\", fontsize=11)\n",
    "axes[1, 2].set_ylabel(\"Frequency\", fontsize=11)\n",
    "axes[1, 2].set_title(\"Test: Distribution Comparison\", fontsize=12)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"beta_regression_performance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# STEP 7: Feature Importance (coefficient magnitudes)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Feature Importance (Coefficient Magnitudes)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "coef_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature\": X_train_const.columns,\n",
    "        \"Coefficient\": results.params,\n",
    "        \"Std_Error\": results.bse,\n",
    "        \"P_Value\": results.pvalues,\n",
    "        \"Abs_Coefficient\": np.abs(results.params),\n",
    "    }\n",
    ").sort_values(\"Abs_Coefficient\", ascending=False)\n",
    "\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# STEP 8: Summary Statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Prediction Summary Statistics\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Dataset\": [\"Train Actual\", \"Train Predicted\", \"Test Actual\", \"Test Predicted\"],\n",
    "        \"Mean\": [\n",
    "            y_train_beta.mean(),\n",
    "            y_pred_train.mean(),\n",
    "            y_test_beta.mean(),\n",
    "            y_pred_test.mean(),\n",
    "        ],\n",
    "        \"Median\": [\n",
    "            y_train_beta.median(),\n",
    "            np.median(y_pred_train),\n",
    "            y_test_beta.median(),\n",
    "            np.median(y_pred_test),\n",
    "        ],\n",
    "        \"Std\": [\n",
    "            y_train_beta.std(),\n",
    "            y_pred_train.std(),\n",
    "            y_test_beta.std(),\n",
    "            y_pred_test.std(),\n",
    "        ],\n",
    "        \"Min\": [\n",
    "            y_train_beta.min(),\n",
    "            y_pred_train.min(),\n",
    "            y_test_beta.min(),\n",
    "            y_pred_test.min(),\n",
    "        ],\n",
    "        \"Max\": [\n",
    "            y_train_beta.max(),\n",
    "            y_pred_train.max(),\n",
    "            y_test_beta.max(),\n",
    "            y_pred_test.max(),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "# Print regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# Plot evaluation for regression\n",
    "plot_model_evaluation_regression(\n",
    "    lin_reg, X_test, y_test, X_train=X_train, model_name=\"Linear Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(random_state=42, n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "# Fit the model\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "# Print regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# Plot evaluation for regression\n",
    "plot_model_evaluation_regression(\n",
    "    xgb_reg, X_test, y_test, X_train=X_train, model_name=\"XGBoost Regressor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "\n",
    "# Fit the model\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_reg.predict(X_test)\n",
    "\n",
    "# Print regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# Plot evaluation for regression\n",
    "plot_model_evaluation_regression(\n",
    "    rf_reg, X_test, y_test, X_train=X_train, model_name=\"Random Forest Regressor\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
