{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "I tilfeller der CumulativeLossAmt er negativ, har kunden betalt for mye. I disse tilfellene settes tapet til 0. Det er snakk om 82 tilfeller. Når tap er høyere enn sum sent, antar jeg at tapet som føres er i forbindelse med renter også? Capper den til sum sent i disse tilfellene. Er snakk om 140 tilfeller.\n",
    "\n",
    "For sakene som er Avsluttet, inkasso oppgjort, må man sjekke om det er betalt inn på slutten. For det er rader, som 334, hvor kunden betaler inn siste måned, men taps-summen står."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "figsize=(14, 4)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import file, plot, data, stat\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, ConfusionMatrixDisplay, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "pd.set_option('display.max_columns', None)\n",
    "data_folder = os.path.join('../../../..', 'data/prod')\n",
    "file_name_collection = \"Collection_data.csv\"\n",
    "file_path_collection = os.path.join(data_folder, file_name_collection)\n",
    "file_name_konto = \"konto_data_trimmed.csv\"\n",
    "file_path_konto = os.path.join(data_folder, file_name_konto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "konto_frame, collection_frame = file.load_konto_data(\n",
    "    file_path_konto\n",
    "), file.load_collection_data(file_path_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_frame[collection_frame[\"PersonId\"] == 1359]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Create target frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_in_kont = konto_frame[\"PersonId\"].unique()\n",
    "collection_frame = collection_frame[collection_frame[\"PersonId\"].isin(ids_in_kont)]\n",
    "target_frame = (\n",
    "    collection_frame[\n",
    "        [\n",
    "            \"Collectionid\",\n",
    "            \"PersonId\",\n",
    "            \"AccountId\",\n",
    "            \"CollectionOpenedDate\",\n",
    "            \"CollectionClosedDate\",\n",
    "            \"BalanceSentAmt\",\n",
    "            \"MonthInDCA\", \"CumulativeLossAmt\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby(\"Collectionid\")\n",
    "    .tail(1)\n",
    "    .set_index(\"Collectionid\")\n",
    ")\n",
    "# Stenger saker når de går til overvåk\n",
    "zcov_frame = collection_frame[collection_frame[\"MonthsInZCOV\"] == 0].copy()\n",
    "zcov_frame = zcov_frame.sort_values(\"YearMonth\").drop_duplicates(\"Collectionid\")\n",
    "zcov_frame[\"ZCOVDate\"] = zcov_frame[\"YearMonth\"] + pd.offsets.MonthEnd(0)\n",
    "target_frame[\"ZCOVDate\"] = target_frame.index.map(\n",
    "    zcov_frame.set_index(\"Collectionid\")[\"ZCOVDate\"]\n",
    ")\n",
    "target_frame[\"ClosedDateSetByZCOVDate\"] = 0\n",
    "cond = target_frame[\"CollectionClosedDate\"].isna() & target_frame[\"ZCOVDate\"].notna()\n",
    "target_frame.loc[cond, \"CollectionClosedDate\"] = target_frame.loc[cond, \"ZCOVDate\"]\n",
    "target_frame.loc[cond, \"ClosedDateSetByZCOVDate\"] = 1\n",
    "\n",
    "# Flagger om sak fortsatt er åpen\n",
    "target_frame[\"IsOpen\"] = (target_frame[\"CollectionClosedDate\"].isna()).astype(int)\n",
    "\n",
    "# Flagger om sak er åpnet innenfor datasettet\n",
    "target_frame[\"CollectionOpenedAfter202309\"] = (\n",
    "    target_frame[\"CollectionOpenedDate\"] >= pd.Timestamp(\"2023-09-01\")\n",
    ").astype(int)\n",
    "\n",
    "# Beregner antall dager saken har vært åpen. Merk at ZCOVDate er satt til siste dag i måneden\n",
    "target_frame[\"DurationDays\"] = (\n",
    "    pd.to_datetime(target_frame[\"CollectionClosedDate\"])\n",
    "    - pd.to_datetime(target_frame[\"CollectionOpenedDate\"])\n",
    ").dt.days\n",
    "\n",
    "# Definerer tap. Tap under 0 settes til 0. Tap over balanse sent settes til balanse\n",
    "target_frame[\"Loss\"] = (\n",
    "    target_frame[\"CumulativeLossAmt\"]\n",
    "    .mask(target_frame[\"CumulativeLossAmt\"] < 0, 0)\n",
    "    .mask(\n",
    "        target_frame[\"CumulativeLossAmt\"] > target_frame[\"BalanceSentAmt\"],\n",
    "        target_frame[\"BalanceSentAmt\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Beregner LGD som en andel av balanse\n",
    "target_frame[\"LGD\"] = target_frame[\"Loss\"] / target_frame[\"BalanceSentAmt\"]\n",
    "\n",
    "# Fjerner alle saker som enten er uten balanse, er åpnet før datasettet, eller ikke er stengt\n",
    "mask_filter = (\n",
    "    (target_frame[\"BalanceSentAmt\"] != 0)\n",
    "    & (target_frame[\"CollectionOpenedAfter202309\"] == 1)\n",
    "    & (target_frame[\"IsOpen\"] == 0)\n",
    ")\n",
    "target_frame = target_frame[mask_filter]\n",
    "\n",
    "# Legger til target som sier om saken har vært mer enn 1 måned til inkasso\n",
    "target_frame[\"LastsMoreThan1MonthInDCA\"] = (target_frame[\"MonthInDCA\"] > 1).astype(int)\n",
    "\n",
    "# Legger til target som sier om saken har vart mer nn 4 uker\n",
    "target_frame[\"LastsMoreThan4Weeks\"] = (target_frame[\"DurationDays\"] > 28).astype(int)\n",
    "\n",
    "# Legger til target som sier om LGD = 0 eller er større\n",
    "target_frame[\"LGD>0\"] = (target_frame[\"LGD\"] > 0).astype(int)\n",
    "target_frame[\"LGD<1\"] = (target_frame[\"LGD\"] < 1).astype(int)\n",
    "target_frame[\"0<LGD<1\"] = (\n",
    "    (target_frame[\"LGD\"] < 1) & (target_frame[\"LGD\"] > 0)\n",
    ").astype(int)\n",
    "\n",
    "# Kolonne man gjør klassifisering på\n",
    "target_col = \"LGD>0\"\n",
    "target_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame[target_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Create feature frame\n",
    "Dataframe of all the konto_frame-rows for each collectionId, based on date opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for idx, row in target_frame.iterrows():\n",
    "    mask = (konto_frame[\"PersonId\"] == row[\"PersonId\"]) & (konto_frame[\"AccountId\"] == row[\"AccountId\"]) & (\n",
    "        konto_frame[\"YearMonth\"] < row[\"CollectionOpenedDate\"]\n",
    "    )\n",
    "    temp = konto_frame[mask].copy()\n",
    "    temp[\"Collectionid\"] = row.name\n",
    "    rows.append(temp)\n",
    "feature_frame = pd.concat(rows, ignore_index=True)\n",
    "feature_frame = data.reorder_column(feature_frame, \"Collectionid\", 3)\n",
    "feature_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Engineer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_map = {\n",
    "    \"Normal\": 0,\n",
    "    \"Ikke aktivert\": 0,\n",
    "    \"Avsluttet\": 0,\n",
    "    \"Avsluttet av kunde\": 0,\n",
    "    #\n",
    "    \"Faktura forfalt\": 1,\n",
    "    \"Purring\": 1,\n",
    "    \"Under avslutning\": 1,\n",
    "    \"Under avslutning manglende KYC\": 1,\n",
    "    \"Sperret\": 1,\n",
    "    \"Sperret, propagert\": 1,\n",
    "    \"Eget misbruk\": 1,\n",
    "    \"Avsluttet inkasso, oppgjort\": 1,\n",
    "    #\n",
    "    \"Purring forfalt\": 2,\n",
    "    \"Purring med kortsperre\": 2,\n",
    "    \"Spesialengasjement, manuell behandling\": 2,\n",
    "    \"Betalingsplan, nedbetaling\": 2,\n",
    "    #\n",
    "    \"Inkassovarsel\": 3,\n",
    "    #\n",
    "    \"Overført inkassobyrå\": 4,\n",
    "    #\n",
    "    \"Inkasso, overført overvåk\": 5,\n",
    "    \"Avsluttet inkasso, med tap\": 5,\n",
    "    \"Gjeldsordning ikke DCA Offentlig\": 5,\n",
    "    \"Gjeldsordning ikke DCA Privat\": 5,\n",
    "    \"Konkursbo Privat\": 5,\n",
    "}\n",
    "feature_frame[\"RisikoStatus\"] = feature_frame[\"GeneralStatusDesc\"].map(status_map)\n",
    "feature_frame = data.reorder_column(feature_frame, \"RisikoStatus\", 10)\n",
    "product_map = {\n",
    "    1: \"SB1 GOLD MC\",\n",
    "    2: \"Sparebank 1 Platinum MC\",\n",
    "    4: \"SH BUSINESS VISA\",\n",
    "    7: \"LOfavør Mastercard\",\n",
    "    8: \"SB1 UNG MC\",\n",
    "    30: \"SH GOLD MC\",\n",
    "    34: \"SB1 EXTRA MC\",\n",
    "    38: \"LOfavør Mastercard Ung\",\n",
    "    40: \"SpareBank 1 Mastercard Elite\",\n",
    "}\n",
    "feature_frame[\"ProductId\"] = feature_frame[\"ProductId\"].map(product_map)\n",
    "feature_frame.rename(columns={\"ProductId\": \"Productname\"}, inplace=True)\n",
    "feature_frame[\"Gender\"] = feature_frame[\"Gender\"].map(\n",
    "    {\"F\": 0, \"M\": 1}\n",
    ").astype(int)\n",
    "feature_frame[\"AgeGroup\"] = (\n",
    "    feature_frame[\"AgeGroup\"]\n",
    "    .map(\n",
    "        {\"0 - 24\": 0, \"25 - 34\": 1, \"35 - 44\": 2, \"45 - 54\": 3, \"55 - 64\": 4, \"> 64\": 5}\n",
    "    )\n",
    "    .astype(int)\n",
    ")\n",
    "col_rename_dict = {\"Gender\": \"Male\"}\n",
    "feature_frame.rename(columns=col_rename_dict, inplace=True)\n",
    "feature_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Combine accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Concatinate into one feature row per Collectionid\n",
    "- Product, siste\n",
    "- Distributør, siste\n",
    "- Aldersgruppe, siste\n",
    "- Male, siste\n",
    "- Risiko-status snitt\n",
    "- Risiko-status antall av hvert tall \n",
    "- Måneder siden konto åpnet, siste\n",
    "- Balanse, siste\n",
    "- Snittbalanse\n",
    "- Balanse standardavvik\n",
    "- Kredittgrense, siste\n",
    "- Antall økninger i kredittgrense\n",
    "- Antall collection-flag = 1\n",
    "- Andel collection-flag = 1\n",
    "- Sum turnover\n",
    "- Turnover som en andel av kredittgrense\n",
    "- Turnover som en andel av balanse\n",
    "- Snitt turnover per num (altså sum av turnover delt på sum av num)\n",
    "- Snitt turnoverNum\n",
    "- TurnoverDom som en andel av Turnover\n",
    "- Sum FundtransferAmt, CashAtmAmt og CashCounterAmt: WithdrawalAmt\n",
    "- TUrnover + Withdrawal som en andel av kredittgrense\n",
    "- OverdueAmt som en andel av StatementClosingBalanceAmt\n",
    "- Andel PaymentOverDueFlag = 1\n",
    "- ANdel RevolvingFlag = 1\n",
    "- Andel av alle kategoriene på TurnoverAmt\n",
    "- Antall rader\n",
    "- Antall måneder siden forrige inkasso-sak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = {\n",
    "\t\"PersonId\": \"last\",\n",
    "\t\"Productname\": \"last\",\n",
    "\t\"DistributorId\": \"last\",\n",
    "\t\"AgeGroup\": \"last\",\n",
    "\t\"Male\": \"last\",\n",
    "\t\"MonthsSinceAccountCreatedNum\": \"last\",\n",
    "\t\"BalanceAmt\": \"last\",\n",
    "\t\"CreditLimitAmt\": \"last\",\n",
    "\t\"OverdueAmt\": \"last\",\n",
    "\t\"StatementClosingBalanceAmt\": \"last\",\n",
    "\t\"Last_Airlines\": \"sum\",\n",
    "    \"Last_Amusement and Entertainment\": \"sum\",\n",
    "    \"Last_Automobile / Vehicle Rental\": \"sum\",\n",
    "    \"Last_Business Services\": \"sum\",\n",
    "    \"Last_Clothing Stores\": \"sum\",\n",
    "    \"Last_Contracted Services\": \"sum\",\n",
    "    \"Last_Government Services\": \"sum\",\n",
    "    \"Last_Hotels\": \"sum\",\n",
    "    \"Last_Includes all lodging merchants\": \"sum\",\n",
    "    \"Last_Mail Order / Telephone Order Providers\": \"sum\",\n",
    "    \"Last_Miscellaneous Stores\": \"sum\",\n",
    "    \"Last_Others\": \"sum\",\n",
    "    \"Last_Professional Services and Membership Organizations\": \"sum\",\n",
    "    \"Last_Repair Services\": \"sum\",\n",
    "    \"Last_Retail Stores\": \"sum\",\n",
    "    \"Last_Service Providers\": \"sum\",\n",
    "    \"Last_Transportation\": \"sum\",\n",
    "    \"Last_Utilities\": \"sum\",\n",
    "    \"Last_Wholesale Distributors and Manufacturers\": \"sum\",\n",
    "\t\n",
    "}\n",
    "feature_copy = feature_frame.copy()\n",
    "agg_frame = (\n",
    "    feature_copy.groupby(\"Collectionid\")\n",
    "    .agg(agg_dict)\n",
    "    .reset_index()\n",
    "    .set_index(\"Collectionid\")\n",
    ")\n",
    "# Antall måneder med data\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"YearMonth\"]\n",
    "    .nunique()\n",
    "    .rename(\"NumberOfMonths\")\n",
    ")\n",
    "# Antall kontoer\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"AccountId\"]\n",
    "    .nunique()\n",
    "    .rename(\"NumberOfAccounts\")\n",
    ")\n",
    "# Snitt av statusene\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"RisikoStatus\"].mean().rename(\"RisikoStatusSnitt\")\n",
    ")\n",
    "# Sum av endringer fra måned til måned\n",
    "agg_frame = agg_frame.join( #Det må håndteres at en person kan ha ulike kontoer. Ha et merge accounts steg\n",
    "    feature_copy.groupby(\"Collectionid\")[\"RisikoStatus\"]\n",
    "    .apply(lambda x: x.diff().abs().sum())\n",
    "    .rename(\"SumRisikoStatusEndringer\")\n",
    ")\n",
    "# Snitt endringer i måneden\n",
    "agg_frame[\"SnittRisikoStatusEndringerPerMåned\"] = (\n",
    "    agg_frame[\"SumRisikoStatusEndringer\"] / agg_frame[\"NumberOfMonths\"]\n",
    ")\n",
    "# Andel av hver risikogruppe\n",
    "risk_counts = (\n",
    "    feature_copy.groupby(\"Collectionid\")[\"RisikoStatus\"]\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "risk_props = risk_counts.div(risk_counts.sum(axis=1), axis=0)\n",
    "risk_props.columns = [f\"RisikoStatus_{col}_share\" for col in risk_props.columns]\n",
    "agg_frame = agg_frame.join(risk_props)\n",
    "# Snitt-balanse\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"BalanceAmt\"]\n",
    "    .mean()\n",
    "    .rename(\"BalanceAmtMean\")\n",
    ")\n",
    "# Standardavvik balanse\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"BalanceAmt\"]\n",
    "    .std()\n",
    "    .fillna(0)\n",
    "    .rename(\"BalanceAmtStd\")\n",
    ")\n",
    "\n",
    "# Antall kredittgrenseøkninger\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"CreditLimitIncreaseFlag\"]\n",
    "    .sum()\n",
    "    .rename(\"AntallCreditLimitIncrease\")\n",
    ")\n",
    "# Antall ganger gått til inkasso\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"CollectionFlag\"]\n",
    "    .sum()\n",
    "    .rename(\"AntallCollectionFlag\")\n",
    ")\n",
    "# Snitt ganger gått til inkasso\n",
    "agg_frame[\"SnittCollectionFlagPerMåned\"] = (\n",
    "    agg_frame[\"AntallCollectionFlag\"] / agg_frame[\"NumberOfMonths\"]\n",
    ")\n",
    "# Sum turnover\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"TurnoverAmt\"]\n",
    "    .sum()\n",
    "    .rename(\"SumTurnover\")\n",
    ")\n",
    "# Snitt turnover\n",
    "agg_frame[\"SnittTurnover\"] = agg_frame[\"SumTurnover\"] / agg_frame[\"NumberOfMonths\"]\n",
    "# Sum antall transaksjoner\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"TurnoverNum\"].sum().rename(\"TurnoverNumSum\")\n",
    ")\n",
    "# Snitt antall transaksjoner\n",
    "agg_frame[\"SnittTurnoverNum\"] = agg_frame[\"TurnoverNumSum\"] / agg_frame[\"NumberOfMonths\"]\n",
    "# Snitt transaksjonsstørrelse\n",
    "agg_frame[\"AverageTransactionSize\"] = (\n",
    "    agg_frame[\"SumTurnover\"] / agg_frame[\"TurnoverNumSum\"]\n",
    ").fillna(0)\n",
    "# Sum innenlandstransaksjoner\n",
    "agg_frame = agg_frame.join( #X\n",
    "    feature_copy.groupby(\"Collectionid\")[\"TurnoverDomAmt\"]\n",
    "    .sum()\n",
    "    .rename(\"SumTurnoverDom\")\n",
    ")\n",
    "agg_frame[\"ShareOfTurnoverIsDomestic\"] = (\n",
    "    agg_frame[\"SumTurnoverDom\"] / agg_frame[\"SumTurnover\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Withdrawal sum\n",
    "agg_frame = agg_frame.join(  # X\n",
    "    feature_copy.groupby(\"Collectionid\")[\"FundtransferAmt\"]\n",
    "    .sum()\n",
    "    .rename(\"SumFundtransfer\")\n",
    ")\n",
    "agg_frame = agg_frame.join(  # X\n",
    "    feature_copy.groupby(\"Collectionid\")[\"CashAtmAmt\"].sum().rename(\"SumCashAtm\")\n",
    ")\n",
    "agg_frame = agg_frame.join(  # X\n",
    "    feature_copy.groupby(\"Collectionid\")[\"CashCounterAmt\"]\n",
    "    .sum()\n",
    "    .rename(\"SumCashCounter\")\n",
    ")\n",
    "agg_frame[\"WithdrawalSum\"] = (\n",
    "    agg_frame[\"SumFundtransfer\"] + agg_frame[\"SumCashAtm\"] + agg_frame[\"SumCashCounter\"]\n",
    ")\n",
    "\n",
    "# Withdrawal propotional to turnover\n",
    "agg_frame[\"WithdrawalPropOfTurnover\"] = (\n",
    "    agg_frame[\"WithdrawalSum\"] / agg_frame[\"SumTurnover\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Total expediture as propotion of kredittgrense\n",
    "agg_frame[\"ExpenditureAsShareOfCreditlimit\"] = (\n",
    "    (agg_frame[\"SumTurnover\"] + agg_frame[\"WithdrawalSum\"]) / agg_frame[\"CreditLimitAmt\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Snitt Turnover andel av kredittgrense\n",
    "feature_copy[\"AndelTurnoverAvKredittgrense\"] = feature_copy[\"TurnoverAmt\"] / feature_copy[\"CreditLimitAmt\"] * -1\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"AndelTurnoverAvKredittgrense\"]\n",
    "    .mean()\n",
    "    .rename(\"SnittTurnoverAndelAvKredittgrense\")\n",
    ")\n",
    "# Siste Balanse som andel av kredittgrense\n",
    "feature_copy[\"AndelBalanseAvKredittgrense\"] = (\n",
    "    feature_copy[\"BalanceAmt\"] / feature_copy[\"CreditLimitAmt\"] * -1\n",
    ")\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"AndelBalanseAvKredittgrense\"]\n",
    "    .last()\n",
    "    .rename(\"LastAndelBalanseAvKredittgrense\")\n",
    ")\n",
    "# Antall overtrekk\n",
    "feature_copy[\"Overtrekk\"] = (\n",
    "    feature_copy[\"InterestEarningLendingAmt\"] > feature_copy[\"CreditLimitAmt\"]\n",
    ").astype(int)\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"Overtrekk\"]\n",
    "    .sum()\n",
    "    .rename(\"AntallOvertrekk\")\n",
    ")\n",
    "# Snitt overtrekk\n",
    "agg_frame[\"SnittOvertrekkPerMåned\"] = (\n",
    "    agg_frame[\"AntallOvertrekk\"] / agg_frame[\"NumberOfMonths\"]\n",
    ")\n",
    "\n",
    "# Andel av overdue kontra størrelse på faktura\n",
    "agg_frame[\"OverdueShareOfDebt\"] = (\n",
    "    agg_frame[\"OverdueAmt\"] / agg_frame[\"StatementClosingBalanceAmt\"]\n",
    ").fillna(0)\n",
    "\n",
    "# Snitt antall payment overdue\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"PaymentOverDueFlag\"]\n",
    "    .mean()\n",
    "    .rename(\"SnittPaymentOverDueFlag\")\n",
    ")\n",
    "# Snitt Revolvingflag\n",
    "agg_frame = agg_frame.join(\n",
    "    feature_copy.groupby(\"Collectionid\")[\"RevolvingFlag\"]\n",
    "    .mean()\n",
    "    .rename(\"SnittRevolvingFlag\")\n",
    ")\n",
    "\n",
    "cols_to_share_by_turnover = [\n",
    "    \"Last_Airlines\",\n",
    "    \"Last_Amusement and Entertainment\",\n",
    "    \"Last_Automobile / Vehicle Rental\",\n",
    "    \"Last_Business Services\",\n",
    "    \"Last_Clothing Stores\",\n",
    "    \"Last_Contracted Services\",\n",
    "    \"Last_Government Services\",\n",
    "    \"Last_Hotels\",\n",
    "    \"Last_Includes all lodging merchants\",\n",
    "    \"Last_Mail Order / Telephone Order Providers\",\n",
    "    \"Last_Miscellaneous Stores\",\n",
    "    \"Last_Others\",\n",
    "    \"Last_Professional Services and Membership Organizations\",\n",
    "    \"Last_Repair Services\",\n",
    "    \"Last_Retail Stores\",\n",
    "    \"Last_Service Providers\",\n",
    "    \"Last_Transportation\",\n",
    "    \"Last_Utilities\",\n",
    "    \"Last_Wholesale Distributors and Manufacturers\",\n",
    "]\n",
    "for col in cols_to_share_by_turnover:\n",
    "    string = re.sub(r\"^Last_\", \"\", col)\n",
    "    string = f\"{string}_share_of_Turnover\"\n",
    "    agg_frame[string] = (agg_frame[col] / agg_frame[\"SumTurnover\"]).fillna(0)\n",
    "\n",
    "\n",
    "cols_to_drop = [\n",
    "    \"SumTurnoverDom\",\n",
    "    \"SumFundtransfer\",\n",
    "    \"SumCashAtm\",\n",
    "    \"SumCashCounter\",\n",
    "] + cols_to_share_by_turnover\n",
    "agg_frame = agg_frame.drop(cols_to_drop, axis=1)\n",
    "agg_frame_encoded = pd.get_dummies(agg_frame, columns=[\"Productname\", \"DistributorId\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_target_frame = agg_frame_encoded.merge(\n",
    "    target_frame.reset_index()[[\"Collectionid\", target_col]],\n",
    "    on=\"Collectionid\",\n",
    "    how=\"left\",\n",
    ")\n",
    "feature_target_frame = feature_target_frame.set_index(\"Collectionid\", drop=True)  # ?\n",
    "feature_target_frame.sort_values(by=\"PersonId\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"PersonId\", target_col]\n",
    "X = feature_target_frame.drop(columns=cols_to_drop)\n",
    "X = X.apply(lambda x: x.astype(int) if x.dtype == \"bool\" else x)\n",
    "y = feature_target_frame[target_col]\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_evaluation_sns(\n",
    "    model, X_test, y_test, X_train=None, top_n_features=10, model_name=\"Model\"\n",
    "):\n",
    "    # --- Predict ---\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = (\n",
    "        model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "    )\n",
    "\n",
    "    # --- Compute metrics ---\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    # --- Feature importances ---\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    else:\n",
    "        importances = None\n",
    "\n",
    "    if importances is not None and X_train is not None:\n",
    "        feat_importance_df = (\n",
    "            pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": importances})\n",
    "            .sort_values(by=\"Importance\", ascending=False)\n",
    "            .head(top_n_features)\n",
    "        )\n",
    "    else:\n",
    "        feat_importance_df = None\n",
    "\n",
    "    # --- Plot subplots ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # 1️⃣ Confusion matrix as heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"Actual\")\n",
    "    axes[0].set_title(f\"{model_name} - Confusion Matrix\")\n",
    "\n",
    "    # 2️⃣ Feature importances\n",
    "    if feat_importance_df is not None:\n",
    "        axes[1].barh(\n",
    "            feat_importance_df[\"Feature\"],\n",
    "            feat_importance_df[\"Importance\"],\n",
    "            color=\"skyblue\",\n",
    "        )\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_xlabel(\"Importance\")\n",
    "        axes[1].set_title(f\"{model_name} - Top {top_n_features} Features\")\n",
    "    else:\n",
    "        axes[1].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No feature importances\",\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "        )\n",
    "        axes[1].set_axis_off()\n",
    "\n",
    "    # 3️⃣ ROC curve\n",
    "    axes[2].plot(fpr, tpr, color=\"blue\", label=f\"AUC = {roc_auc:.2f}\")\n",
    "    axes[2].plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\")\n",
    "    axes[2].set_xlabel(\"False Positive Rate\")\n",
    "    axes[2].set_ylabel(\"True Positive Rate\")\n",
    "    axes[2].set_title(f\"{model_name} - ROC Curve\")\n",
    "    axes[2].legend(loc=\"lower right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_model_evaluation_sns(\n",
    "    log_reg, X_test, y_test, X_train=X_train, model_name=\"Logistic Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,  # number of trees\n",
    "    max_depth=3,  # tree depth\n",
    "    learning_rate=0.1,  # step size shrinkage\n",
    "    subsample=0.8,  # row sampling\n",
    "    colsample_bytree=0.8,  # column sampling\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_model_evaluation_sns(\n",
    "    xgb_model, X_test, y_test, X_train=X_train, model_name=\"XGBoost\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Med hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_model_evaluation_sns(rf, X_test, y_test, X_train=X_train, model_name=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "        \"n_estimators\": [100, 200, 500],\n",
    "        \"max_depth\": [None, 5, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_model_evaluation_sns(\n",
    "    best_rf, X_test, y_test, X_train=X_train, model_name=\"Random forest\"\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
